import asyncio
from playwright.async_api import async_playwright
import pandas as pd
from datetime import datetime

class MyTCASScraper:
    def __init__(self, base_url="https://course.mytcas.com"):
        self.base_url = base_url
        self.collected_data = []

    async def perform_search(self, page, keyword):
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô"""
        try:
            print(f"\nüîé Searching for keyword: '{keyword}'")
            await page.goto(self.base_url, wait_until='networkidle')
            await asyncio.sleep(2)

            # ‡∏´‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡πÇ‡∏î‡∏¢‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ selectors ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
            search_input = None
            possible_selectors = [
                "input[placeholder='‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢ ‡∏Ñ‡∏ì‡∏∞ ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£']",
                "input[type='search']",
                "input.search-input",
                "input[aria-label*='‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤']",
            ]
            for sel in possible_selectors:
                try:
                    search_input = await page.query_selector(sel)
                    if search_input:
                        print(f"  ‚úîÔ∏è Found search input by selector: {sel}")
                        break
                except:
                    continue
            if not search_input:
                print("  ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
                return []

            # ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            await search_input.fill("")
            await asyncio.sleep(0.5)
            await search_input.fill(keyword)
            await search_input.press("Enter")
            await asyncio.sleep(3)

            # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ selector)
            program_list = []
            result_selectors = [
                "ul.t-programs > li",
                "ul.program-list > li",
                "div.search-results > ul > li",
                "[data-testid='program-item']",
            ]
            for sel in result_selectors:
                try:
                    items = await page.query_selector_all(sel)
                    if items and len(items) > 0:
                        print(f"  ‚úîÔ∏è ‡∏û‡∏ö {len(items)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ selector: {sel}")
                        program_list = items
                        break
                except:
                    continue
            if not program_list:
                print("  ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")
                return []

            programs = []
            for idx, item in enumerate(program_list):
                try:
                    text = await item.inner_text()
                    link_el = await item.query_selector("a")
                    if not link_el:
                        continue
                    href = await link_el.get_attribute("href")
                    full_url = href if href.startswith("http") else self.base_url + href

                    lines = [line.strip() for line in text.splitlines() if line.strip()]
                    program_name = lines[0] if len(lines) > 0 else ""
                    faculty = lines[1].replace('‚Ä∫', ' > ') if len(lines) > 1 else ""
                    university = lines[2] if len(lines) > 2 else ""

                    programs.append({
                        "keyword": keyword,
                        "program_name": program_name,
                        "faculty": faculty,
                        "university": university,
                        "url": full_url,
                        "raw_text": text
                    })
                    print(f"  {idx+1}. {program_name[:50]}...")

                except Exception as e:
                    print(f"  ‚ö†Ô∏è Error processing item #{idx+1}: {e}")

            return programs

        except Exception as e:
            print(f"‚ùå Error during searching '{keyword}': {e}")
            return []

    async def fetch_program_details(self, page, program):
        """‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£"""
        try:
            print(f"üìÑ Fetching details for: {program['program_name'][:50]}...")
            await page.goto(program["url"], wait_until="networkidle")
            await asyncio.sleep(2)

            details = {
                "keyword": program["keyword"],
                "program_name": program["program_name"],
                "faculty": program["faculty"],
                "university": program["university"],
                "program_type": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                "tuition_fee": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                "url": program["url"],
                "scrape_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

            # Selector ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à
            type_selectors = [
                "dt:has-text('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£') + dd",
                ".program-type",
                "[data-field='program_type']",
                "td:has-text('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£') + td"
            ]
            fee_selectors = [
                "dt:has-text('‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢') + dd",
                "dt:has-text('‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°') + dd",
                ".fee-info",
                ".tuition-fee",
                "[data-field='fee']",
                "td:has-text('‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢') + td"
            ]

            for sel in type_selectors:
                el = await page.query_selector(sel)
                if el:
                    txt = (await el.inner_text()).strip()
                    if txt:
                        details["program_type"] = txt
                        break

            for sel in fee_selectors:
                el = await page.query_selector(sel)
                if el:
                    txt = (await el.inner_text()).strip()
                    if txt:
                        details["tuition_fee"] = txt
                        break

            # ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            try:
                rows = await page.query_selector_all("table tr")
                for row in rows:
                    cells = await row.query_selector_all("td, th")
                    if len(cells) >= 2:
                        header = (await cells[0].inner_text()).strip()
                        value = (await cells[1].inner_text()).strip()

                        if "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó" in header and details["program_type"] == "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•":
                            details["program_type"] = value
                        if any(w in header for w in ["‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô"]) and details["tuition_fee"] == "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•":
                            details["tuition_fee"] = value
            except:
                pass

            print(f"   ‚úîÔ∏è {details['university'][:30]} - {details['tuition_fee'][:30]}")
            return details

        except Exception as e:
            print(f"   ‚ùå Failed to fetch details: {e}")
            return None

    async def scrape(self, keywords):
        """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏Ñ‡∏£‡∏±‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        results = []
        print("üöÄ Starting scraping process...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô False ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡πá‡∏ô browser
            context = await browser.new_context(
                locale="th-TH",
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            for kw in keywords:
                programs = await self.perform_search(page, kw)
                if not programs:
                    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô '{kw}'")
                    continue

                for idx, program in enumerate(programs, 1):
                    print(f"[{idx}/{len(programs)}] Processing program...")
                    detail = await self.fetch_program_details(page, program)
                    if detail:
                        self.collected_data.append(detail)
                    await asyncio.sleep(1.2)  # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Ñ

            await browser.close()

        print(f"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£: {len(self.collected_data)}")
        return self.collected_data

    def save_data(self, filename_prefix="mytcas_scraped"):
        if not self.collected_data:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            return None
        df = pd.DataFrame(self.collected_data)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_file = f"{filename_prefix}_{timestamp}.xlsx"
        csv_file = f"{filename_prefix}_{timestamp}.csv"

        df.to_excel(excel_file, index=False, engine='openpyxl')
        df.to_csv(csv_file, index=False, encoding="utf-8-sig")

        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel: {excel_file}")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV: {csv_file}")
        return df

async def main():
    print("=== MyTCAS Scraper ===")
    print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô:")
    print("1) ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå")
    print("2) ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå")
    print("3) ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏≥")
    print("4) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á")

    choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1-4): ").strip()
    if choice == "1":
        keywords = ["‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå"]
    elif choice == "2":
        keywords = ["‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå"]
    elif choice == "3":
        keywords = ["‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå", "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå"]
    elif choice == "4":
        kw_input = input("‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ , ): ")
        keywords = [k.strip() for k in kw_input.split(",") if k.strip()]
    else:
        print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á, ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô '‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå'")
        keywords = ["‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏° ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå"]

    print(f"üîé ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {keywords}")

    scraper = MyTCASScraper()
    await scraper.scrape(keywords)
    scraper.save_data()

if __name__ == "__main__":
    asyncio.run(main())
